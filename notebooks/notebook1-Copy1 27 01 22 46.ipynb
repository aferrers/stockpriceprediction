{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28c4fce-f9c3-408e-a300-e89f6afe17db",
   "metadata": {},
   "source": [
    "## Data import and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797ae9ca-106d-4c4c-a622-837ffcc4877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab07a22e-fd65-4451-b03f-c7528968355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install yfinance and import Data\n",
    "import yfinance as yf\n",
    "from yfinance import Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a6f563-3144-487d-8bf7-c09aec3e5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for yfinance data\n",
    "tickers = ['AAPL', 'GOOGL', 'MSFT', 'IBM']\n",
    "start_date = datetime.strptime('01 01 2013', '%d %m %Y')\n",
    "end_date = datetime.strptime('31 12 2023', '%d %m %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8682cfc4-0ec2-420f-afbb-85a048af4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain data\n",
    "dfs = {}\n",
    "for ticker in tickers:\n",
    "    ticker_obj = Ticker(ticker)\n",
    "    history = ticker_obj.history(start=start_date, end=end_date)\n",
    "    dfs[ticker] = pd.DataFrame(history)\n",
    "    #dfs[ticker].index = dfs[ticker].index.date\n",
    "    #dfs[ticker].index = dfs[ticker].index.strftime('%Y-%m-%d')\n",
    "    #print(f\"{ticker} closing prices: {history['Close']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a475e60-fc30-490a-af0b-38bf66d8ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from yfinance import Ticker\n",
    "from datetime import datetime\n",
    "\n",
    "def get_stock_data(tickers, start_date, end_date, selected_variable):\n",
    "    start_date = datetime.strptime(start_date, '%d %m %Y')\n",
    "    print(start_date)\n",
    "    end_date = datetime.strptime(end_date, '%d %m %Y')\n",
    "    print(end_date)\n",
    "\n",
    "    dfs = {}\n",
    "    for ticker in tickers:\n",
    "        ticker_obj = Ticker(ticker)\n",
    "        history = ticker_obj.history(start=start_date, end=end_date)\n",
    "        dfs[ticker] = pd.DataFrame(history)\n",
    "        dfs[ticker] = dfs[ticker][selected_variable]\n",
    "        \n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88eaf91d-a161-43d4-a6cb-42211ea265c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-01-01 00:00:00\n",
      "2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "tickers = ['AAPL', 'GOOGL', 'MSFT', 'IBM', 'NVDA']\n",
    "start_date = '01 01 2013'\n",
    "end_date = '31 12 2023'\n",
    "selected_variable = 'Close'\n",
    "stock_data_dfs = get_stock_data(tickers, start_date, end_date, selected_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b89d2025-6c63-47c1-a39f-2763fd57ab6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2013-01-02 00:00:00-05:00     16.769093\n",
       "2013-01-03 00:00:00-05:00     16.557434\n",
       "2013-01-04 00:00:00-05:00     16.096226\n",
       "2013-01-07 00:00:00-05:00     16.001545\n",
       "2013-01-08 00:00:00-05:00     16.044609\n",
       "                                ...    \n",
       "2023-12-22 00:00:00-05:00    193.600006\n",
       "2023-12-26 00:00:00-05:00    193.050003\n",
       "2023-12-27 00:00:00-05:00    193.149994\n",
       "2023-12-28 00:00:00-05:00    193.580002\n",
       "2023-12-29 00:00:00-05:00    192.529999\n",
       "Name: Close, Length: 2768, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_dfs['AAPL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb999a-0ee5-488c-b43a-a84e87b64b2a",
   "metadata": {},
   "source": [
    "### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46198ceb-416d-49f2-935c-11f622682682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe\n",
    "for ticker in tickers:\n",
    "    stock_data_dfs[ticker] = pd.DataFrame(stock_data_dfs[ticker])\n",
    "    stock_data_dfs[ticker].to_csv(f'../data/raw_{ticker}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504b6c2-76dd-45d4-845c-da85543304d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe\n",
    "for ticker in tickers:\n",
    "    stock_data_dfs[ticker] = pd.DataFrame(dfs['AAPL'])\n",
    "df_googl = pd.DataFrame(dfs['GOOGL'])\n",
    "df_msft = pd.DataFrame(dfs['MSFT'])\n",
    "df_ibm = pd.DataFrame(dfs['IBM'])\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_aapl.to_csv('../data/raw_aapl.csv', index=False)\n",
    "df_googl.to_csv('../data/raw_googl.csv', index=False)\n",
    "df_msft.to_csv('../data/raw_msft.csv', index=False)\n",
    "df_ibm.to_csv('../data/raw_ibm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e1877-08f8-47e8-99b9-048cccb7e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibm_close = pd.DataFrame(dfs['IBM']['Close'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98785273-af63-4efe-b917-cfa96c4f9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ibm = df_ibm.set_index('Date', inplace=False)\n",
    "#df_ibm.columns = df_ibm.columns.droplevel(0)\n",
    "#df_ibm = df_ibm.reset_index(inplace=False)\n",
    "\n",
    "df_ibm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f28e6-122b-4f6b-8653-58deeeabfa5b",
   "metadata": {},
   "source": [
    "### Missing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75bbae-f7cf-48df-81cc-c73212fe0a74",
   "metadata": {},
   "source": [
    "how to handle missing values?\n",
    "as the stock market is closed on weekends, the values will be carried forward from the previous Friday's close price/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d222e-54ec-4b1d-9868-c3fdae5a62c0",
   "metadata": {},
   "source": [
    "other options:\n",
    " \n",
    " Rolling Statistics Imputation: This method substitutes missing values with a rolling statistic (like mean, median, or mode) over a specified window period. This method can handle non-random missingness and preserve temporal dependence, but the choice of window size and statistic can significantly affect the results 1.\n",
    " \n",
    " Using pandas for rolling statistics imputation\n",
    " \n",
    "window_size = 5 # adjust this value according to your needs\n",
    "\n",
    "df_imputed = df.rolling(window_size).mean().fillna(df)\n",
    "\n",
    "Interpolation: This method estimates missing values based on the surrounding values. Linear interpolation is suitable for linear trends, while polynomial interpolation can capture more complex trends. However, these methods can distort the data if the trend is not linear or if there's a seasonal component.\n",
    "\n",
    "Using pandas for linear interpolation\n",
    "df_imputed = df.interpolate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463532c-b165-41ab-aaad-a37a4b1d9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete date range for the period covered by the data\n",
    "complete_dates = pd.date_range(start=df_ibm_close.index.min(), end=df_ibm_close.index.max())\n",
    "complete_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f73c8-84f2-4399-bd86-1483821dfefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data with the complete date range\n",
    "merged_df = df_ibm.reindex(complete_dates)\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25906627-5566-4c2a-9ae9-0a6c7504f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing dates\n",
    "missing_dates = merged_df[merged_df['Close'].isnull()].index\n",
    "len(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207425f7-531e-4aae-8176-4ae077b41b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates_df = pd.DataFrame(missing_dates, columns=['date'])\n",
    "missing_dates_df['weekday'] = missing_dates_df['date'].dt.weekday\n",
    "\n",
    "# Count the number of missing dates for each weekday\n",
    "missing_counts = missing_dates_df.groupby('weekday').size().reset_index()\n",
    "missing_counts.columns = ['weekday', 'nulls']\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc4a7c-7b97-4174-bd91-b1c2a1627166",
   "metadata": {},
   "source": [
    "NAs treatment: forward fill NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b7fc0-aa11-4ae6-b9a9-6663bbe74196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filled_nas = missing_counts.fillna(method='ffill', inplace=False)\n",
    "# remove hour from formatting?\n",
    "\n",
    "clean_ibm = merged_df.ffill(inplace=False)\n",
    "clean_ibm.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a7e1c-ebf2-4afd-b200-a085322864ed",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d8927-eddc-43b1-8816-76f5340f1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving clean data:\n",
    "clean_ibm.to_csv('../data/clean_ibm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bee628-e043-4624-a5a8-efcf296ff0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = clean_ibm.index.max()\n",
    "time_days = pd.Timedelta(days=365)\n",
    "start_time_range = max_date - time_range\n",
    "\n",
    "clean_ibm_filtered = clean_ibm[clean_ibm.index >= start_time_range]\n",
    "\n",
    "#clean_ibm_filtered = clean_ibm.iloc[3648:4013]\n",
    "clean_ibm_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad7611-ee28-4d19-b843-5add5e5f6775",
   "metadata": {},
   "source": [
    "## Initial visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb4b8f-dad2-48c6-9d52-522e1778022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ibm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76c590-8ac0-4eca-bb4f-8885a174bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(clean_ibm_filtered)\n",
    "plt.title('Stock Price Time Series Plot')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80449648-08ce-4491-82cd-cf9db23022bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(clean_ibm_filtered, kde=True)\n",
    "plt.title('Histogram')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260dba8-172f-4629-b677-7a3f464ca8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "seasonal_decompose_result = seasonal_decompose(clean_ibm_filtered)\n",
    "seasonal_decompose_result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f7107-070d-4861-bf89-b1ec40022a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86556814-b38d-4416-b655-f4930a3bde01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33bc4b-f8a2-4a86-a6a0-f891a08770ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of months in data\n",
    "num_months = clean_ibm_filtered.index.to_period('M').nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99d906-d273-4422-bbde-f6d94d94df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data test\n",
    "unique_years = clean_ibm_filtered.index.year.unique().tolist()\n",
    "print(unique_years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896c2a9-a488-4155-b387-964e9eeba4e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### visualization tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66becd78-df65-48bc-9141-0a2c481a34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to be fixed, shwoign 1970 instead of 2022. ISO DATES?\n",
    "#https://matplotlib.org/stable/gallery/text_labels_and_annotations/date.html\n",
    "\n",
    "import datetime\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cbook as cbook\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "fig, ax = plot_series(clean_ibm_filtered)\n",
    "\n",
    "#date_fmt = mdates.DateFormatter('%Y-%m')\n",
    "#date_fmt = mdates.DateFormatter('%Y-%m', mdates.iso_date)\n",
    "#ax.xaxis.set_major_formatter(date_fmt)\n",
    "\n",
    "#ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))\n",
    "#ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "#ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8caeba-1833-47da-8705-2e1b037c1d6f",
   "metadata": {},
   "source": [
    "## Hypothesis testing: stationary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50940b0e-679c-4b18-80e8-1cf91c734b71",
   "metadata": {},
   "source": [
    "We will observe and test the data to discover if it's stationary or not by:  \n",
    "a) visual tests  \n",
    "b) summary statistics by partition  \n",
    "c) statistical tests  \n",
    "    -- Dickey-Fuller (DF) test   \n",
    "    -- Kwiatkowski-Phillips-Schmidt-Shin (KPSS)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee41750-bde2-419a-99f2-482c96460cb1",
   "metadata": {},
   "source": [
    "### Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cffa19-f8e4-4862-a66c-a882b0d88714",
   "metadata": {},
   "source": [
    "Dickey-Fuller Test\n",
    "The Dickey-Fuller (DF) test was developed and popularized by Dickey and Fuller (1979). The null hypothesis of DF test is that there is a unit root in an AR model, which implies that the data series is not stationary. The alternative hypothesis is generally stationarity or trend stationarity but can be different depending on the version of the test is being used.\n",
    "\n",
    "Since the null hypothesis assumes the presence of unit root, that is α=1, the p-value obtained should be less than the significance level (say 0.05) in order to reject the null hypothesis. Thereby, inferring that the series is stationary.\n",
    "\n",
    "parameters:  \n",
    "“c” : constant only (default).\n",
    "“ct” : constant and trend.\n",
    "“ctt” : constant, and linear and quadratic trend.\n",
    "“n” : no constant, no trend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580fa3c-b830-42c9-bf54-c604daed315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# 10Y data: clean_ibm\n",
    "# 1Y data: clean_ibm_filtered\n",
    "# Perform the ADF test\n",
    "adfuller_test = adfuller(\n",
    "    clean_ibm,\n",
    "    maxlag=None, \n",
    "    regression='c', \n",
    "    autolag='AIC', \n",
    "    store=False, \n",
    "    regresults=False)\n",
    "\n",
    "\n",
    "\n",
    "print('ADF Statistic: %f' % adfuller_test[0])\n",
    "print('p-value: %f' % adfuller_test[1])\n",
    "print('used_lag: %f' % adfuller_test[2])\n",
    "print('nobs: %f' % adfuller_test[3])\n",
    "for key, value in adfuller_test[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3da2a-6b23-48f6-9b67-8514e82726ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# 10Y data: clean_ibm\n",
    "# 1Y data: clean_ibm_filtered\n",
    "# Perform the ADF test\n",
    "adfuller_test = adfuller(\n",
    "    clean_ibm_filtered,\n",
    "    maxlag=None, \n",
    "    regression='c', \n",
    "    autolag='AIC', \n",
    "    store=False, \n",
    "    regresults=False)\n",
    "\n",
    "\n",
    "\n",
    "print('ADF Statistic: %f' % adfuller_test[0])\n",
    "print('p-value: %f' % adfuller_test[1])\n",
    "print('used_lag: %f' % adfuller_test[2])\n",
    "print('nobs: %f' % adfuller_test[3])\n",
    "for key, value in adfuller_test[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd451c4-4b42-46e3-a2c9-7c1dbe99d196",
   "metadata": {},
   "source": [
    "Null - hypothesis:  \n",
    "No reason found to reject the null hypothesis found as the p-value is greater than the significance level  \n",
    "therefore this test assumes the data is non-stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec777c6-28eb-4ffd-9a88-6616c029e6e8",
   "metadata": {},
   "source": [
    "### KPSS test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf137670-315d-4a49-9ed0-a54875590dfc",
   "metadata": {},
   "source": [
    "The KPSS test, short for, Kwiatkowski-Phillips-Schmidt-Shin (KPSS), is a type of Unit root test that tests for the stationarity of a given series around a deterministic trend. In other words, the test is somewhat similar in spirit to the ADF test. A common misconception, however, is that it can be used interchangeably with the ADF test. This can lead to misinterpretations about stationarity, which can easily go undetected, causing more problems down the line.  \n",
    "source: https://www.analyticsvidhya.com/blog/2021/06/statistical-tests-to-check-stationarity-in-time-series-part-1/#:~:text=That%20is%2C%20if%20the%20p%2Dvalue%20is%20%3C%20significance%20level,the%20tested%20series%20is%20stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd615e11-09d8-4262-9733-d0655692627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "kpss_test = kpss(\n",
    "    clean_ibm_filtered,\n",
    "    regression='c', \n",
    "    nlags='auto', \n",
    "    store=False)\n",
    "print('kpss stat: %f' % kpss_test[0])\n",
    "print('p-value: %f' % kpss_test[1])\n",
    "print('used_lag: %f' % kpss_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866f51e-3cf1-4e53-b3e9-b70a7be7d5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bd92ae4-f60f-49d3-aeb4-1091e46e2662",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c200cc4-107a-43f7-a54d-1ad4370599f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "def kpss_test(timeseries):\n",
    "    print(\"Results of KPSS Test:\")\n",
    "    kpsstest = kpss(timeseries, regression=\"c\", nlags=\"auto\")\n",
    "    kpss_output = pd.Series(\n",
    "        kpsstest[0:3], index=[\"Test Statistic\", \"p-value\", \"Lags Used\"]\n",
    "    )\n",
    "    for key, value in kpsstest[0](https://stackoverflow.com/questions/49738705/time-series-data-kpss-test-result).items():\n",
    "        kpss_output[\"Critical Value (%s)\" % key] = value\n",
    "    print(kpss_output)\n",
    "\n",
    "# Assuming `clean_ibm` is a Pandas DataFrame with a DateTimeIndex\n",
    "kpss_test(clean_ibm['your_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a5e7c-29ce-4200-9d36-218a95e8197f",
   "metadata": {},
   "source": [
    "### other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ffb79-0e58-4960-90cc-59700b82b20b",
   "metadata": {},
   "source": [
    "background data:\n",
    "\n",
    "Visual Inspection: Plot the data and visually inspect it for any obvious trends. A trend exists if the general direction of the data changes over time. A deterministic trend is typically linear and can be modeled with a well-defined mathematical function, meaning that the long-term behavior of the time series is predictable.\n",
    "\n",
    "Autocorrelation Function (ACF) Plots: ACF plots can help identify whether a time series is a random walk or has a deterministic trend. For a random walk, the ACF plot will have a single spike at lag 1 and fade away quickly after that. For a deterministic trend, the ACF plot will have significant lags beyond 1.\n",
    "\n",
    "Statistical Tests: Use statistical tests such as the Augmented Dickey-Fuller (ADF) test to formally test the stationarity of the data. If the p-value resulting from the test is less than your chosen significance level (often 0.05), you reject the null hypothesis and conclude that the series is stationary. If the p-value is greater than your significance level, you fail to reject the null hypothesis and conclude that the series is non-stationary .\n",
    "\n",
    "Detrending: If the time series exhibits a deterministic trend, you can detrend the data by regressing the series on a high-order polynomial function of time. The order of the polynomial can be determined by t-tests and F-tests as well as AIC and SBC measures of fit.\n",
    "\n",
    "Differencing: If the time series is a random walk, you can make it stationary by differencing. This involves subtracting the previous observation from the current observation. Note that differencing will reduce the length of the series by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abc06b-0251-42e1-8c1d-98d391cf3cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a2bba7-fd39-4899-8332-b4d560b28696",
   "metadata": {},
   "source": [
    "info:  \n",
    "https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html  \n",
    "https://pandas.pydata.org/docs/reference/api/pandas.Series.autocorr.html  \n",
    "post predictions:  \n",
    "https://www.investopedia.com/terms/d/durbin-watson-statistic.asp  \n",
    "https://www.statsmodels.org/dev/generated/statsmodels.stats.stattools.durbin_watson.html (residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c97fda-1528-41b5-872d-5e03cc061a97",
   "metadata": {},
   "source": [
    "### Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3311c-48d4-4429-8206-0a3b0c4eca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is a DataFrame with a DateTimeIndex and a single column of numeric data\n",
    "x = np.arange(clean_ibm_filtered.index.size)\n",
    "\n",
    "# Fit a linear regression\n",
    "coefficients = np.polyfit(x, clean_ibm_filtered.iloc[:, 0], deg=1)\n",
    "\n",
    "# Print the slope (first coefficient)\n",
    "print(\"Slope: \", coefficients[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47173621-3156-4242-8b28-c83aefc1fad8",
   "metadata": {},
   "source": [
    "How to consider this trend?  \n",
    "create a de-trender anyway and create a pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7796506-ca0e-494b-b0a1-8d300e6a39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(clean_ibm_filtered['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2ed01-8cd1-4968-83c8-5420897dfc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d3d95b-9ab1-4155-abb2-e0d6ddef49d7",
   "metadata": {},
   "source": [
    "### autocorrelation (non-stationary data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe3161-3374-4942-b423-464e75e2c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Autocorrelation plot\n",
    "x = pd.plotting.autocorrelation_plot(clean_ibm_filtered)\n",
    " \n",
    "# plotting the Curve\n",
    "x.plot()\n",
    " \n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7145b-d59b-48ed-b2d6-e12c6d1c9b41",
   "metadata": {},
   "source": [
    "correlation check ideas:  \n",
    "Autocorrelation Function (ACF): The ACF plot can help identify seasonality. If there is significant seasonality, the ACF plot should show spikes at lags equal to the period.  \n",
    "Statistical Tests: There are formal hypothesis tests available to detect seasonality, such as the Student T-Test and the Wilcoxon Signed Rank Test. These tests can be used to check if the observed seasonality is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbaf671-e678-4e55-8408-19850747cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(clean_ibm_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931eae5-2159-4c12-9aee-2aa838164a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ibm_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dba5f7-ec87-401e-b7b1-3dca0db2949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(clean_ibm_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200c3d3-423d-4c59-a700-d3f1c0099b33",
   "metadata": {},
   "source": [
    "### autocorrelation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d63779-e853-4ec1-8b44-0b6f42707270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding plot title.\n",
    "plt.title(\"Autocorrelation Plot\") \n",
    " \n",
    "# Providing x-axis name.\n",
    "plt.xlabel(\"Lags\") \n",
    " \n",
    "# Plotting the Autocorrelation plot.\n",
    "if isinstance(filled_nas, pd.DataFrame):\n",
    "    filled_nas_val = filled_nas.iloc[:, 0] # Select the first column as a Series\n",
    "elif isinstance(filled_nas, pd.Series):\n",
    "    filled_nas_val = filled_nas\n",
    "else:\n",
    "    filled_nas_val = filled_nas\n",
    "\n",
    "plt.acorr(filled_nas_val, maxlags = 10) \n",
    " \n",
    "# Displaying the plot.\n",
    "print(\"The Autocorrelation plot for the data is:\")\n",
    "plt.grid(True)\n",
    " \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87990a7-9602-4c8c-8b9e-e4e8d3c2063b",
   "metadata": {},
   "source": [
    "## Converting non - stationary data to stationary. \n",
    "differencing, log transformation, or a Box-Cox transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a73068-b18d-4e40-9f4a-ec3f9127f788",
   "metadata": {},
   "source": [
    "Statistical stationarity: A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., \"stationarized\") through the use of mathematical transformations. A stationarized series is relatively easy to predict: you simply predict that its statistical properties will be the same in the future as they have been in the past!   (Recall our famous forecasting quotes.)  The predictions for the stationarized series can then be \"untransformed,\" by reversing whatever mathematical transformations were previously used, to obtain predictions for the original series. (The details are normally taken care of by your software.) Thus, finding the sequence of transformations needed to stationarize a time series often provides important clues in the search for an appropriate forecasting model.  Stationarizing a time series through differencing (where needed) is an important part of the process of fitting an ARIMA model, as discussed in the ARIMA pages of these notes.\n",
    "\n",
    "Another reason for trying to stationarize a time series is to be able to obtain meaningful sample statistics such as means, variances, and correlations with other variables. Such statistics are useful as descriptors of future behavior only if the series is stationary. For example, if the series is consistently increasing over time, the sample mean and variance will grow with the size of the sample, and they will always underestimate the mean and variance in future periods. And if the mean and variance of a series are not well-defined, then neither are its correlations with other variables. For this reason you should be cautious about trying to extrapolate regression models fitted to nonstationary data.\n",
    "\n",
    "Most business and economic time series are far from stationary when expressed in their original units of measurement, and even after deflation or seasonal adjustment they will typically still exhibit trends, cycles, random-walking, and other non-stationary behavior.   If the series has a stable long-run trend and tends to revert to the trend line following a disturbance, it may be possible to stationarize it by de-trending (e.g., by fitting a trend line and subtracting it out prior to fitting a model, or else by including the time index as an independent variable in a regression or ARIMA model), perhaps in conjunction with logging or deflating.   Such a series is said to be trend-stationary.    However, sometimes even de-trending is not sufficient to make the series stationary, in which case it may be necessary to transform it into a series of period-to-period and/or season-to-season differences.  If the mean, variance, and autocorrelations of the original series are not constant in time, even after detrending, perhaps the statistics of the changes in the series between periods or between seasons will be constant.   Such a series is said to be difference-stationary.  (Sometimes it can be hard to tell the difference between a series that is trend-stationary and one that is difference-stationary, and a so-called unit root test may be used to get a more definitive answer.  We will return to this topic later in the course.)\n",
    "(Return to top of page.)\n",
    "source: https://people.duke.edu/~rnau/411diff.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5d531-c001-4125-9857-0e9d9a14fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ibm_filtered_diff = clean_ibm_filtered.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85066d11-e6ae-4a35-80eb-cd4957976fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ibm_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363afc27-6c81-4bbd-b17b-0d94deb7d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ibm_filtered_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19e6ab-f24b-4ae6-9c2a-d943b51d1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(clean_ibm_filtered_diff)\n",
    "plt.title('Stock Price Time Series Plot')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02993c95-ff17-49ca-ba8d-cb67a4650c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(clean_ibm_filtered_diff, kde=True)\n",
    "plt.title('Histogram')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3ad3c-e5cf-4238-9649-53dd351b2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(clean_ibm_filtered_diff)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a8c65-b048-4059-bc72-c1f5cc20a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ibm_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3f8f4-372d-4523-a57d-fcd9b06ba986",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ibm_filtered_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05f455-35f8-4ab6-b804-2987c305d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# 10Y data: clean_ibm\n",
    "# 1Y data: clean_ibm_filtered\n",
    "# 1Y data diff: clean_ibm_filtered_diff\n",
    "\n",
    "# Perform the ADF test\n",
    "adfuller_test = adfuller(\n",
    "    clean_ibm_filtered_diff,\n",
    "    maxlag=None, \n",
    "    regression='n', \n",
    "    autolag='AIC', \n",
    "    store=False, \n",
    "    regresults=False)\n",
    "\n",
    "\n",
    "\n",
    "print('ADF Statistic: %f' % adfuller_test[0])\n",
    "print('p-value: %f' % adfuller_test[1])\n",
    "print('used_lag: %f' % adfuller_test[2])\n",
    "print('nobs: %f' % adfuller_test[3])\n",
    "for key, value in adfuller_test[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669cbffe-77ff-4c32-a40a-3a6b64d5d9c2",
   "metadata": {},
   "source": [
    "### autocorrelation (diff data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b4f4c-d62b-4203-9aeb-792ba7a5231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Autocorrelation plot\n",
    "x = pd.plotting.autocorrelation_plot(clean_ibm_filtered_diff)\n",
    " \n",
    "# plotting the Curve\n",
    "x.plot()\n",
    " \n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c670161-f1e9-44f9-8fd4-69708c577ba5",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f7277-3fd2-478d-aac7-6910eb7bb55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import SlidingWindowSplitter\n",
    "#from sktime.forecasting.naive import NaiveForecaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885f425-0338-4333-a041-990172eb10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = SlidingWindowSplitter(fh=1, window_length=10, step_length=1)\n",
    "window_size = 12 #define window size\n",
    "sws = SlidingWindowSplitter(\n",
    "    window_length=window_size,\n",
    "    step_length=1) #set splitter\n",
    "\n",
    "train_indices, test_indices = [], []\n",
    "for train_index, test_index in sws.split(clean_ibm_filtered_diff):\n",
    "    train_indices.append(train_index)\n",
    "    test_indices.append(test_index)\n",
    "\n",
    "sws.get_fh()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f55ca3-96e4-433d-a06d-84e9af4ae271",
   "metadata": {},
   "outputs": [],
   "source": [
    "sws.get_fh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25236355-6b4a-4e95-b97d-ed7d00f98551",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_indices[0]\n",
    "y_test = test_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab2a5b-ff16-46c4-8ac9-db8f29ee8d4d",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906dcffa-fc6c-46b2-a397-908539540903",
   "metadata": {},
   "source": [
    "For example, in python and R, the auto ARIMA method itself will generate the optimal p and q parameters, which would be suitable for the data set to provide better forecasting. The high-level logic behind that is the same as the logic behind hyperparameter tuning of any other machine learning model. We need to try some combinations of p and q parameters and compare results using a validation set.\n",
    "\n",
    "Since our search space is not big, usually values p and q are not higher than 10, we can apply a popular technique for hyperparameter optimization called grid search. Grid search is simply an exhaustive search through a manually specified subset of the hyperparameter space of a learning algorithm. Basically, it means that this method will try each combination of p and q from the specified subset that we provided.  \n",
    "source: https://www.baeldung.com/cs/acf-pacf-plots-arma-modeling#:~:text=The%20autocorrelation%20function%20(ACF)%20is,number%20of%20periods%20or%20units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65ea95-e56a-4b5b-9319-ea66703deb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA + gridearch SARIMA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca95b3-d501-445c-8c0f-1fbcce94ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bf17a-b938-4231-aabf-c90a85cc3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check multiple train test splits from sktime!\n",
    "#https://towardsdatascience.com/build-complex-time-series-regression-pipelines-with-sktime-910bc25c96b6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a8526-a35d-415a-9f67-1cb9e5ece6c3",
   "metadata": {},
   "source": [
    "### Naive Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b5bca-8bf9-42be-ab87-c759080039dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ibm_filtered_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b757a0-1621-40cc-876c-f7cb4fb33c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e8e9e9-0dbe-4494-a274-4f72601bde96",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65fd41-5d87-4e13-b32c-655dc3e584cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pmdarima import AutoARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea641073-6c55-476f-addf-129e2a8de22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d241f23c-e88e-479a-ac5e-887f3a209f0d",
   "metadata": {},
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#series.index = series.index.to_period('M')\n",
    "\n",
    "# fit model\n",
    "model_arima = ARIMA(clean_ibm_filtered_diff, order=(5,0,0))\n",
    "\n",
    "model_arima = ARIMA(\n",
    "    clean_ibm_filtered_diff, \n",
    "    exog=None, \n",
    "    order=(5, 0, 0), \n",
    "    seasonal_order=(0, 0, 0, 0), \n",
    "    trend=None, \n",
    "    enforce_stationarity=True, \n",
    "    enforce_invertibility=True, \n",
    "    concentrate_scale=False, \n",
    "    trend_offset=1, \n",
    "    dates=None, \n",
    "    freq=None, \n",
    "    missing='none', \n",
    "    validate_specification=True)\n",
    "\n",
    "\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83344b6b-ba10-4bc2-bd84-2d5be874e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of fit model\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874c9d9-00fc-466d-9847-1b1b0219d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot of residuals\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "# density plot of residuals\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "# summary stats of residuals\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd492bce-6333-4d8b-8155-832ec60bcc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f043e40-1f56-4421-8ea5-2d1a1f93b842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9194cc-5530-4aa9-aa0f-f63b0920a495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
